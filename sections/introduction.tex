\section{Introduction}
\label{chap:introduction}

In 2016 the world heard about a computer program named AlphaGo that in October 2015
won five to zero with Fan Hui, a Go professional and three-time European Champion. Later,
in March 2016 that same AI system won four to one with Lee Sedol, widely considered the
greatest player of that decade. Those revelations were groundbreaking considering that Go
was regarded as a major challenge for artificial intelligence. And yet AlphaGo managed not
only to master this game but also to devise innovative moves unknown to humans before \cite{AlphaGoBlog}.

Since then, we have noticed rapid development in this area. A crucial milestone was
the creation of AlphaGo Zero which, in contrast to its predecessors, used only reinforcement
learning during training and yet managed to outperform all the older versions in only forty
days \cite{AlphaGoZeroBlog}. Moreover, while the original AlphaGo algorithm used a complex setup
of four different neural networks for its decision-making, and required historical human games
for the training process, the AlphaGo Zero algorithm used only a single residual neural network,
and learned solely by self-play. Furthermore, the input size of AlphaGo Zero was also significantly
reduced by eliminating all hand-crafted features and leaving only the essential elements,
thus eliminating the need for any external knowledge during the learning process.

The next major milestone was the creation of AlphaZero. While this algorithm was largely similar
to its predecessor, it included modifications that allowed the generalisation to other games besides Go,
such as allowing for non-binary game outcomes and discontinuing the use of board rotations as a form of
data augmentation. That opened up opportunities for AI to master other games like Chess or Shogi.

As a result, the question arises whether it is possible to create an artificial intelligence
agent that can repeat AlphaGo and AlphaGo Zero's success in the area of more complex
board games like \textit{Settlers of Catan}, \textit{Pandemic}, \textit{Azul}, etc. This 
project attempts to answer this question by preparing an agent for the \textit{Carcassonne} game 
using reinforcement learning and analysing its behaviour and ability to make optimal 
decisions in a dynamic environment.

The following sections present the achieved goals. The section `Game engine architecture' 
presents the architecture of the game engine, its components and communication within the entire system. `Agent architecture' 
concentrates on describing the structure of the game agent and neural network input and
output structure. Next, the section `Evaluation' describes a design of the conducted experiment 
and the analysis of the agent's performance and its strategies.
